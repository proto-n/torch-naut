{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing CRPS/MDN models with and without epistemic uncertainty methods\n",
    "\n",
    "In this notebook, we compare different methods with and without epistemic uncertainty modeling. We implement and evaluate four models: a basic CRPS model, a CRPS ensemble, a Mixture Density Network (MDN), and an MDN with Bayesian Neural Network (BNN) parameters. Using the diabetes dataset as an example, we demonstrate how to incorporate epistemic uncertainty through ensembling and variational inference, and compare their performance using CRPS, negative log-likelihood (NLL), and RMSE metrics. The notebook shows how TorchNaut makes it easy to experiment with different uncertainty quantification approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, loading and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchnaut import mdn, utils, kde, crps, epistemic\n",
    "from torchnaut.utils import LabelScaler\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target.reshape(-1, 1) # Note: reshape y to 2D.\n",
    "\n",
    "# Split into train (70%), validation (15%), and test (15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features using training data only\n",
    "scaler_X = StandardScaler().fit(X_train)\n",
    "X_train = scaler_X.transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# The utils.LabelScaler class provides a convenient way to standardize targets, with the ability to\n",
    "# also inverse transform tensors of shape [batch_size, n_samples, n_features]\n",
    "scaler_y = LabelScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and optimizers\n",
    "\n",
    "At this point we are comparing a number of models, so we need some modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRPS Model\n",
    "class CRPS_Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            crps.EpsilonSampler(16), \n",
    "            nn.Linear(64 + 16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, n_samples=100):\n",
    "        with crps.EpsilonSampler.n_samples(n_samples):\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "    # methods for templated training and evaluation:\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loss(model, batch_X, batch_y):\n",
    "        outputs = model(batch_X)\n",
    "        return crps.crps_loss(outputs, batch_y).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def validation_loss(model, batch_X, batch_y, scaler_y):\n",
    "        outputs = model(batch_X)\n",
    "        outputs_scaled = scaler_y.inverse_transform(outputs.unsqueeze(-1)).squeeze()\n",
    "        return crps.crps_loss(outputs_scaled, batch_y)\n",
    "\n",
    "    @staticmethod\n",
    "    def test_eval(model, batch_X, batch_y, scaler_y):\n",
    "        outputs = model(batch_X)\n",
    "        outputs_scaled = scaler_y.inverse_transform(outputs.unsqueeze(-1)).squeeze()\n",
    "\n",
    "        crps_loss = crps.crps_loss(outputs_scaled, batch_y)\n",
    "        with torch.device(outputs.device):\n",
    "            nll = kde.nll_gpu(outputs_scaled, batch_y)\n",
    "        pred_mean = outputs_scaled.mean(dim=-1)\n",
    "\n",
    "        return nll, crps_loss, pred_mean\n",
    "\n",
    "models[\"CRPS\"] = CRPS_Model(input_dim=X_train.shape[1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRPS Ensemble Model\n",
    "class CRPS_Ensemble_Model(epistemic.CRPSEnsemble):\n",
    "    # most of the logic is already implemented in the parent class\n",
    "    \n",
    "    # methods for templated training and evaluation:\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loss(model, batch_X, batch_y):\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # ensemble outputs are given as [batch_size x n_ensemble x n_samples] and we\n",
    "        # apply crps loss to each model individually (i.e., last dimension as usual)\n",
    "        return crps.crps_loss(outputs, batch_y.unsqueeze(-2)).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def validation_loss(model, batch_X, batch_y, scaler_y):\n",
    "        outputs = model(batch_X)\n",
    "        outputs_scaled = scaler_y.inverse_transform(outputs.unsqueeze(-1)).squeeze()\n",
    "\n",
    "        # for validation and testing, we need to flatten the last two dimensions to\n",
    "        # get a sample from the mixture distribution of the component models\n",
    "        outputs_scaled = outputs_scaled.flatten(start_dim=-2)\n",
    "        return crps.crps_loss(outputs_scaled, batch_y)\n",
    "\n",
    "    @staticmethod\n",
    "    def test_eval(model, batch_X, batch_y, scaler_y):\n",
    "        outputs = model(batch_X)\n",
    "        outputs_scaled = scaler_y.inverse_transform(outputs.unsqueeze(-1)).squeeze()\n",
    "        outputs_scaled = outputs_scaled.flatten(start_dim=-2)\n",
    "\n",
    "        crps_loss = crps.crps_loss(outputs_scaled, batch_y)\n",
    "        with torch.device(outputs.device):\n",
    "            nll = kde.nll_gpu(outputs_scaled, batch_y)\n",
    "        pred_mean = outputs_scaled.mean(dim=-1)\n",
    "\n",
    "        return nll, crps_loss, pred_mean\n",
    "    \n",
    "models[\"CRPS Ensemble\"] = CRPS_Ensemble_Model([\n",
    "    CRPS_Model(input_dim=X_train.shape[1]).to(device) for _ in range(5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDN model\n",
    "class MDN_Model(nn.Module):\n",
    "    def __init__(self, input_dim, n_components=100):\n",
    "        super().__init__()\n",
    "        self.mdn = mdn.MDN(n_components=n_components) # Helper class\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.mdn.network_output_dim), # MDN output parameters\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # methods for templated training and evaluation:\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loss(model, batch_X, batch_y):\n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        nll = (-1) * model.mdn.get_dist(outputs).log_prob(batch_y.squeeze())\n",
    "        return torch.clamp(nll, min=-20).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def validation_loss(model, batch_X, batch_y, scaler_y):\n",
    "        outputs = model(batch_X)\n",
    "        outputs_scaled = model.mdn.inverse_transform(outputs, scaler_y).squeeze(-1)\n",
    "        nll = (-1) * model.mdn.get_dist(outputs_scaled).log_prob(batch_y.squeeze())\n",
    "        return nll\n",
    "\n",
    "    @staticmethod\n",
    "    def test_eval(model, batch_X, batch_y, scaler_y):\n",
    "        outputs = model(batch_X)\n",
    "        outputs_scaled = model.mdn.inverse_transform(outputs, scaler_y).squeeze(-1)\n",
    "        samples = model.mdn.get_dist(outputs_scaled).sample((1000,)).T\n",
    "\n",
    "        crps_loss = crps.crps_loss(samples, batch_y)\n",
    "        pred_mean = model.mdn.get_dist(outputs_scaled).mean\n",
    "        nll = (-1) * model.mdn.get_dist(outputs_scaled).log_prob(batch_y.squeeze())\n",
    "        return nll, crps_loss, pred_mean\n",
    "\n",
    "models[\"MDN\"] = MDN_Model(input_dim=X_train.shape[1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDN model with Bayesian Neural Network\n",
    "class MDN_BNN_Model(nn.Module):\n",
    "    def __init__(self, input_dim, n_components=100, kl_weight=0.01):\n",
    "        super().__init__()\n",
    "        self.mdn = mdn.MDN(n_components=n_components)\n",
    "        self.kl_weight = kl_weight\n",
    "        self.layers = nn.ModuleList([\n",
    "            epistemic.BayesianLinear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            epistemic.BayesianLinear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            epistemic.BayesianLinear(32, self.mdn.network_output_dim),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loss(model, batch_X, batch_y):\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # BNN training is done by just doing backprop on a single forward pass\n",
    "        nll = (-1) * model.mdn.get_dist(outputs).log_prob(batch_y.squeeze())\n",
    "        # However, we also need KL divergence loss for the BNN\n",
    "        kl_loss = epistemic.get_kl_term(model)\n",
    "        return torch.clamp(nll, min=-20).mean() + model.kl_weight * kl_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def validation_loss(model, batch_X, batch_y, scaler_y):\n",
    "        # BNN validation is done by sampling from the posterior\n",
    "        num_evals = 10\n",
    "        lls = []\n",
    "        for i in range(num_evals):\n",
    "            outputs = model(batch_X)\n",
    "            outputs_scaled = model.mdn.inverse_transform(outputs, scaler_y).squeeze()\n",
    "            lls.append(model.mdn.get_dist(outputs_scaled).log_prob(batch_y.squeeze()))\n",
    "\n",
    "        # We need to average the likelihoods in a numerically stable way\n",
    "        nll = (-1)*(\n",
    "            torch.logsumexp(\n",
    "                torch.stack(lls, dim=0),\n",
    "                dim=0,\n",
    "            ) - torch.log(torch.tensor(num_evals))\n",
    "        )\n",
    "\n",
    "        return nll\n",
    "\n",
    "    @staticmethod\n",
    "    def test_eval(model, batch_X, batch_y, scaler_y):\n",
    "        # BNN validation is done by sampling from the posterior\n",
    "        num_evals = 50\n",
    "        lls = []\n",
    "        samples = []\n",
    "        means = []\n",
    "\n",
    "        for i in range(num_evals):\n",
    "            outputs = model(batch_X)\n",
    "            outputs_scaled = model.mdn.inverse_transform(outputs, scaler_y).squeeze()\n",
    "            pred_samples = model.mdn.get_dist(outputs_scaled).sample((1000,)).T\n",
    "            pred_mean = model.mdn.get_dist(outputs_scaled).mean\n",
    "\n",
    "            lls.append(model.mdn.get_dist(outputs_scaled).log_prob(batch_y.squeeze()))\n",
    "            means.append(pred_mean)\n",
    "            samples.append(pred_samples)\n",
    "    \n",
    "        pred_mean = torch.stack(means, dim=0).mean(dim=0)\n",
    "\n",
    "        nll = (-1)*(\n",
    "            torch.logsumexp(\n",
    "                torch.stack(lls, dim=0),\n",
    "                dim=0,\n",
    "            ) - torch.log(torch.tensor(num_evals))\n",
    "        )\n",
    "    \n",
    "        # Mixture of samples\n",
    "        crps_loss = crps.crps_loss(torch.concatenate(samples, dim=-1), batch_y)\n",
    "\n",
    "        return nll, crps_loss, pred_mean\n",
    "\n",
    "# The theory suggest a kl_weight of 1/training_set_size, however in practice it seems that\n",
    "# any nonzero kl_weight always results in a worse performing model.\n",
    "models[\"MDN BNN\"] = MDN_BNN_Model(input_dim=X_train.shape[1], kl_weight=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr=0.001):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=5)\n",
    "\n",
    "    # Training loop with validation\n",
    "    epochs = 100\n",
    "    best_val_loss = np.inf\n",
    "    best_model = None\n",
    "    patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model.train_loss(model, batch_X, batch_y)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                val_loss = model.validation_loss(model, batch_X, batch_y, scaler_y)\n",
    "                val_losses.append(val_loss.cpu().numpy())\n",
    "            val_loss = np.concatenate(val_losses).mean()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        if best_val_loss > val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 9:\n",
    "                print(\"Early stopping!\")\n",
    "                model.load_state_dict(best_model)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.8233, Val Loss: 59.9787\n",
      "Epoch [2/100], Train Loss: 0.8147, Val Loss: 59.6076\n",
      "Epoch [3/100], Train Loss: 0.8108, Val Loss: 59.0376\n",
      "Epoch [4/100], Train Loss: 0.8156, Val Loss: 57.9708\n",
      "Epoch [5/100], Train Loss: 0.7983, Val Loss: 56.7538\n",
      "Epoch [6/100], Train Loss: 0.7877, Val Loss: 55.2330\n",
      "Epoch [7/100], Train Loss: 0.7650, Val Loss: 53.6087\n",
      "Epoch [8/100], Train Loss: 0.7480, Val Loss: 52.0432\n",
      "Epoch [9/100], Train Loss: 0.7310, Val Loss: 50.1892\n",
      "Epoch [10/100], Train Loss: 0.7317, Val Loss: 48.2144\n",
      "Epoch [11/100], Train Loss: 0.6915, Val Loss: 46.1995\n",
      "Epoch [12/100], Train Loss: 0.6688, Val Loss: 44.5743\n",
      "Epoch [13/100], Train Loss: 0.6613, Val Loss: 42.7342\n",
      "Epoch [14/100], Train Loss: 0.6416, Val Loss: 40.2690\n",
      "Epoch [15/100], Train Loss: 0.6236, Val Loss: 39.0085\n",
      "Epoch [16/100], Train Loss: 0.5995, Val Loss: 37.0422\n",
      "Epoch [17/100], Train Loss: 0.5759, Val Loss: 35.5175\n",
      "Epoch [18/100], Train Loss: 0.5560, Val Loss: 34.1464\n",
      "Epoch [19/100], Train Loss: 0.5377, Val Loss: 32.6131\n",
      "Epoch [20/100], Train Loss: 0.5092, Val Loss: 31.8057\n",
      "Epoch [21/100], Train Loss: 0.5112, Val Loss: 30.6808\n",
      "Epoch [22/100], Train Loss: 0.4842, Val Loss: 30.1086\n",
      "Epoch [23/100], Train Loss: 0.4813, Val Loss: 29.2951\n",
      "Epoch [24/100], Train Loss: 0.4579, Val Loss: 28.9708\n",
      "Epoch [25/100], Train Loss: 0.4570, Val Loss: 28.3388\n",
      "Epoch [26/100], Train Loss: 0.4483, Val Loss: 28.0625\n",
      "Epoch [27/100], Train Loss: 0.4318, Val Loss: 27.5303\n",
      "Epoch [28/100], Train Loss: 0.4211, Val Loss: 27.2441\n",
      "Epoch [29/100], Train Loss: 0.4232, Val Loss: 26.7654\n",
      "Epoch [30/100], Train Loss: 0.4172, Val Loss: 26.8120\n",
      "Epoch [31/100], Train Loss: 0.4244, Val Loss: 27.4003\n",
      "Epoch [32/100], Train Loss: 0.4137, Val Loss: 27.2893\n",
      "Epoch [33/100], Train Loss: 0.4087, Val Loss: 26.7703\n",
      "Epoch [34/100], Train Loss: 0.4166, Val Loss: 26.7973\n",
      "Epoch [35/100], Train Loss: 0.4011, Val Loss: 26.3040\n",
      "Epoch [36/100], Train Loss: 0.4071, Val Loss: 27.0752\n",
      "Epoch [37/100], Train Loss: 0.4032, Val Loss: 26.7971\n",
      "Epoch [38/100], Train Loss: 0.3951, Val Loss: 26.9454\n",
      "Epoch [39/100], Train Loss: 0.4021, Val Loss: 26.8971\n",
      "Epoch [40/100], Train Loss: 0.3964, Val Loss: 26.8989\n",
      "Epoch [41/100], Train Loss: 0.3992, Val Loss: 25.9260\n",
      "Epoch [42/100], Train Loss: 0.3981, Val Loss: 26.2082\n",
      "Epoch [43/100], Train Loss: 0.3955, Val Loss: 27.1691\n",
      "Epoch [44/100], Train Loss: 0.3966, Val Loss: 27.0212\n",
      "Epoch [45/100], Train Loss: 0.3964, Val Loss: 27.2582\n",
      "Epoch [46/100], Train Loss: 0.3983, Val Loss: 27.4123\n",
      "Epoch [47/100], Train Loss: 0.3873, Val Loss: 26.0938\n",
      "Epoch [48/100], Train Loss: 0.3909, Val Loss: 27.0579\n",
      "Epoch [49/100], Train Loss: 0.3978, Val Loss: 27.0425\n",
      "Epoch [50/100], Train Loss: 0.3838, Val Loss: 26.1471\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "train(models[\"CRPS\"], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.5578, Val Loss: 9.7773\n",
      "Epoch [2/100], Train Loss: 1.5521, Val Loss: 9.6779\n",
      "Epoch [3/100], Train Loss: 1.5345, Val Loss: 9.5565\n",
      "Epoch [4/100], Train Loss: 1.5498, Val Loss: 9.3632\n",
      "Epoch [5/100], Train Loss: 1.5425, Val Loss: 9.0799\n",
      "Epoch [6/100], Train Loss: 1.5382, Val Loss: 8.7556\n",
      "Epoch [7/100], Train Loss: 1.5011, Val Loss: 8.4014\n",
      "Epoch [8/100], Train Loss: 1.5278, Val Loss: 8.1186\n",
      "Epoch [9/100], Train Loss: 1.4936, Val Loss: 7.7884\n",
      "Epoch [10/100], Train Loss: 1.4489, Val Loss: 7.5522\n",
      "Epoch [11/100], Train Loss: 1.4546, Val Loss: 7.3057\n",
      "Epoch [12/100], Train Loss: 1.4374, Val Loss: 7.1236\n",
      "Epoch [13/100], Train Loss: 1.4170, Val Loss: 6.9355\n",
      "Epoch [14/100], Train Loss: 1.3942, Val Loss: 6.7836\n",
      "Epoch [15/100], Train Loss: 1.3792, Val Loss: 6.6755\n",
      "Epoch [16/100], Train Loss: 1.3567, Val Loss: 6.5542\n",
      "Epoch [17/100], Train Loss: 1.3507, Val Loss: 6.4471\n",
      "Epoch [18/100], Train Loss: 1.3285, Val Loss: 6.3828\n",
      "Epoch [19/100], Train Loss: 1.3292, Val Loss: 6.3472\n",
      "Epoch [20/100], Train Loss: 1.2984, Val Loss: 6.2751\n",
      "Epoch [21/100], Train Loss: 1.2975, Val Loss: 6.2772\n",
      "Epoch [22/100], Train Loss: 1.2735, Val Loss: 6.2029\n",
      "Epoch [23/100], Train Loss: 1.2497, Val Loss: 6.1992\n",
      "Epoch [24/100], Train Loss: 1.2437, Val Loss: 6.2343\n",
      "Epoch [25/100], Train Loss: 1.2067, Val Loss: 6.2454\n",
      "Epoch [26/100], Train Loss: 1.1962, Val Loss: 6.2449\n",
      "Epoch [27/100], Train Loss: 1.1820, Val Loss: 6.2481\n",
      "Epoch [28/100], Train Loss: 1.1625, Val Loss: 6.3142\n",
      "Epoch [29/100], Train Loss: 1.1511, Val Loss: 6.3380\n",
      "Epoch [30/100], Train Loss: 1.1320, Val Loss: 6.4325\n",
      "Epoch [31/100], Train Loss: 1.1131, Val Loss: 6.4301\n",
      "Epoch [32/100], Train Loss: 1.1074, Val Loss: 6.6124\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "train(models[\"MDN\"], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.7840, Val Loss: 53.6423\n",
      "Epoch [2/100], Train Loss: 0.7873, Val Loss: 53.4073\n",
      "Epoch [3/100], Train Loss: 0.7793, Val Loss: 52.8570\n",
      "Epoch [4/100], Train Loss: 0.7720, Val Loss: 52.1343\n",
      "Epoch [5/100], Train Loss: 0.7466, Val Loss: 51.1202\n",
      "Epoch [6/100], Train Loss: 0.7512, Val Loss: 49.8888\n",
      "Epoch [7/100], Train Loss: 0.7286, Val Loss: 48.5927\n",
      "Epoch [8/100], Train Loss: 0.7142, Val Loss: 46.7620\n",
      "Epoch [9/100], Train Loss: 0.6798, Val Loss: 45.0625\n",
      "Epoch [10/100], Train Loss: 0.6827, Val Loss: 43.3226\n",
      "Epoch [11/100], Train Loss: 0.6495, Val Loss: 41.3851\n",
      "Epoch [12/100], Train Loss: 0.6421, Val Loss: 39.6103\n",
      "Epoch [13/100], Train Loss: 0.6225, Val Loss: 37.7615\n",
      "Epoch [14/100], Train Loss: 0.5861, Val Loss: 35.9893\n",
      "Epoch [15/100], Train Loss: 0.5591, Val Loss: 34.1865\n",
      "Epoch [16/100], Train Loss: 0.5435, Val Loss: 32.8487\n",
      "Epoch [17/100], Train Loss: 0.5148, Val Loss: 31.2191\n",
      "Epoch [18/100], Train Loss: 0.5017, Val Loss: 30.2618\n",
      "Epoch [19/100], Train Loss: 0.4860, Val Loss: 29.2746\n",
      "Epoch [20/100], Train Loss: 0.4654, Val Loss: 28.5170\n",
      "Epoch [21/100], Train Loss: 0.4618, Val Loss: 27.9835\n",
      "Epoch [22/100], Train Loss: 0.4548, Val Loss: 27.4138\n",
      "Epoch [23/100], Train Loss: 0.4451, Val Loss: 27.3712\n",
      "Epoch [24/100], Train Loss: 0.4343, Val Loss: 26.9315\n",
      "Epoch [25/100], Train Loss: 0.4199, Val Loss: 26.8637\n",
      "Epoch [26/100], Train Loss: 0.4162, Val Loss: 26.7698\n",
      "Epoch [27/100], Train Loss: 0.4164, Val Loss: 26.4922\n",
      "Epoch [28/100], Train Loss: 0.4105, Val Loss: 26.5306\n",
      "Epoch [29/100], Train Loss: 0.4134, Val Loss: 26.2255\n",
      "Epoch [30/100], Train Loss: 0.4133, Val Loss: 26.2451\n",
      "Epoch [31/100], Train Loss: 0.4051, Val Loss: 26.1993\n",
      "Epoch [32/100], Train Loss: 0.3961, Val Loss: 26.3002\n",
      "Epoch [33/100], Train Loss: 0.3922, Val Loss: 26.1832\n",
      "Epoch [34/100], Train Loss: 0.3868, Val Loss: 26.4594\n",
      "Epoch [35/100], Train Loss: 0.3901, Val Loss: 26.2851\n",
      "Epoch [36/100], Train Loss: 0.3878, Val Loss: 26.1087\n",
      "Epoch [37/100], Train Loss: 0.3889, Val Loss: 26.4433\n",
      "Epoch [38/100], Train Loss: 0.3899, Val Loss: 26.3656\n",
      "Epoch [39/100], Train Loss: 0.3917, Val Loss: 26.5380\n",
      "Epoch [40/100], Train Loss: 0.3878, Val Loss: 26.5447\n",
      "Epoch [41/100], Train Loss: 0.3850, Val Loss: 26.5465\n",
      "Epoch [42/100], Train Loss: 0.3864, Val Loss: 26.4720\n",
      "Epoch [43/100], Train Loss: 0.3851, Val Loss: 26.7886\n",
      "Epoch [44/100], Train Loss: 0.3824, Val Loss: 26.6009\n",
      "Epoch [45/100], Train Loss: 0.3851, Val Loss: 26.6553\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "train(models[\"CRPS Ensemble\"], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.5075, Val Loss: 7.1795\n",
      "Epoch [2/100], Train Loss: 1.5035, Val Loss: 7.1583\n",
      "Epoch [3/100], Train Loss: 1.5205, Val Loss: 7.1344\n",
      "Epoch [4/100], Train Loss: 1.5137, Val Loss: 7.1187\n",
      "Epoch [5/100], Train Loss: 1.5124, Val Loss: 7.0804\n",
      "Epoch [6/100], Train Loss: 1.4523, Val Loss: 7.0249\n",
      "Epoch [7/100], Train Loss: 1.4669, Val Loss: 6.8752\n",
      "Epoch [8/100], Train Loss: 1.4735, Val Loss: 6.8369\n",
      "Epoch [9/100], Train Loss: 1.4532, Val Loss: 6.8497\n",
      "Epoch [10/100], Train Loss: 1.4582, Val Loss: 6.7065\n",
      "Epoch [11/100], Train Loss: 1.4617, Val Loss: 6.7093\n",
      "Epoch [12/100], Train Loss: 1.4220, Val Loss: 6.6346\n",
      "Epoch [13/100], Train Loss: 1.4459, Val Loss: 6.6052\n",
      "Epoch [14/100], Train Loss: 1.4232, Val Loss: 6.5564\n",
      "Epoch [15/100], Train Loss: 1.4247, Val Loss: 6.5590\n",
      "Epoch [16/100], Train Loss: 1.4236, Val Loss: 6.5264\n",
      "Epoch [17/100], Train Loss: 1.4195, Val Loss: 6.5078\n",
      "Epoch [18/100], Train Loss: 1.4012, Val Loss: 6.4634\n",
      "Epoch [19/100], Train Loss: 1.3856, Val Loss: 6.3981\n",
      "Epoch [20/100], Train Loss: 1.3835, Val Loss: 6.3062\n",
      "Epoch [21/100], Train Loss: 1.3984, Val Loss: 6.2644\n",
      "Epoch [22/100], Train Loss: 1.3871, Val Loss: 6.2615\n",
      "Epoch [23/100], Train Loss: 1.3676, Val Loss: 6.2731\n",
      "Epoch [24/100], Train Loss: 1.3506, Val Loss: 6.2656\n",
      "Epoch [25/100], Train Loss: 1.3585, Val Loss: 6.2186\n",
      "Epoch [26/100], Train Loss: 1.3388, Val Loss: 6.1951\n",
      "Epoch [27/100], Train Loss: 1.3467, Val Loss: 6.1792\n",
      "Epoch [28/100], Train Loss: 1.3268, Val Loss: 6.1731\n",
      "Epoch [29/100], Train Loss: 1.3484, Val Loss: 6.1201\n",
      "Epoch [30/100], Train Loss: 1.3357, Val Loss: 6.1177\n",
      "Epoch [31/100], Train Loss: 1.3136, Val Loss: 6.0647\n",
      "Epoch [32/100], Train Loss: 1.3152, Val Loss: 6.0792\n",
      "Epoch [33/100], Train Loss: 1.3119, Val Loss: 6.0657\n",
      "Epoch [34/100], Train Loss: 1.3067, Val Loss: 6.0459\n",
      "Epoch [35/100], Train Loss: 1.2999, Val Loss: 6.1146\n",
      "Epoch [36/100], Train Loss: 1.2774, Val Loss: 6.0352\n",
      "Epoch [37/100], Train Loss: 1.2781, Val Loss: 6.0955\n",
      "Epoch [38/100], Train Loss: 1.2754, Val Loss: 5.9821\n",
      "Epoch [39/100], Train Loss: 1.2769, Val Loss: 6.0417\n",
      "Epoch [40/100], Train Loss: 1.2729, Val Loss: 6.0164\n",
      "Epoch [41/100], Train Loss: 1.2473, Val Loss: 6.0406\n",
      "Epoch [42/100], Train Loss: 1.2485, Val Loss: 5.9927\n",
      "Epoch [43/100], Train Loss: 1.2293, Val Loss: 6.0595\n",
      "Epoch [44/100], Train Loss: 1.2324, Val Loss: 6.0723\n",
      "Epoch [45/100], Train Loss: 1.2106, Val Loss: 6.0348\n",
      "Epoch [46/100], Train Loss: 1.2177, Val Loss: 6.0836\n",
      "Epoch [47/100], Train Loss: 1.2349, Val Loss: 6.0540\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "train(models[\"MDN BNN\"], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Test CRPS: 43.0078\n",
      "Marginal Test NLL: 5.6368\n",
      "Marginal Test RMSE: 166.5705)\n"
     ]
    }
   ],
   "source": [
    "marginal_point_pred = X_train_tensor.mean()\n",
    "marginal_dist_pred = torch.tensor(y_train).T.to(device)\n",
    "\n",
    "marginal_crps = []\n",
    "marginal_nll = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        with torch.device(device):\n",
    "            mnll = kde.nll_gpu(marginal_dist_pred.repeat(batch_y.shape[0], 1), batch_y.to(device))\n",
    "            marginal_nll.append(mnll.cpu().numpy())\n",
    "        mcrps = crps.crps_loss(marginal_dist_pred.repeat(batch_y.shape[0], 1), batch_y.to(device)).cpu()\n",
    "        marginal_crps.append(mcrps)\n",
    "    test_crps_marginal = np.concatenate(marginal_crps).mean()\n",
    "    test_nll_marginal = np.concatenate(marginal_nll).mean()\n",
    "\n",
    "print(f\"Marginal Test CRPS: {test_crps_marginal:.4f}\")\n",
    "print(f\"Marginal Test NLL: {test_nll_marginal:.4f}\")\n",
    "print(f\"Marginal Test RMSE: {((y_test_tensor-marginal_point_pred)**2).mean()**(1/2):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS Test CRPS: 31.0463\n",
      "CRPS Test NLL: 5.4289\n",
      "CRPS Test RMSE: 86.8475\n",
      "CRPS Ensemble Test CRPS: 30.6537\n",
      "CRPS Ensemble Test NLL: 5.3919\n",
      "CRPS Ensemble Test RMSE: 85.9982\n",
      "MDN Test CRPS: 38.3105\n",
      "MDN Test NLL: 7.2156\n",
      "MDN Test RMSE: 79.8511\n",
      "MDN BNN Test CRPS: 38.9359\n",
      "MDN BNN Test NLL: 6.3563\n",
      "MDN BNN Test RMSE: 77.6576\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    test_crps = []\n",
    "    test_rmse = []\n",
    "    test_nll = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            nll, crps_loss, pred_mean = model.test_eval(model, batch_X.to(device), batch_y.to(device), scaler_y)\n",
    "            test_crps.append(crps_loss.cpu().numpy())\n",
    "            test_nll.append(nll.cpu().numpy())\n",
    "            test_rmse.append(((batch_y - pred_mean.cpu().numpy()) ** 2).mean(dim=-1).cpu().numpy())\n",
    "        test_crps = np.concatenate(test_crps).mean()\n",
    "        test_nll = np.concatenate(test_nll).mean()\n",
    "        test_rmse = np.sqrt(np.concatenate(test_rmse).mean())\n",
    "\n",
    "    print(f\"{name} Test CRPS: {test_crps:.4f}\")\n",
    "    print(f\"{name} Test NLL: {test_nll:.4f}\")\n",
    "    print(f\"{name} Test RMSE: {test_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
